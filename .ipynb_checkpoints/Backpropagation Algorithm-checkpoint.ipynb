{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e287fe1-612a-4a4d-a3fa-d1a13d72210c",
   "metadata": {},
   "source": [
    "# Backpropagation Algorithm\n",
    "\n",
    "In this notebook I implement backpropagation algorithm. See section *C*,  chapter $6$ of this [paper](https://arxiv.org/pdf/1803.08823)\n",
    "cite [paper](https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf)\n",
    "\n",
    "In this notebook I attempt to reproduce the results of the first expriement done in [learning representations by back-propagating errors ](https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf). \n",
    "\n",
    "**Backpropagation Algorithm:**\n",
    "\n",
    "- [ ] Add the defintions of the weights and the activation ..etc..  \n",
    "Gradient Decent allows us to identify the set of weights which optimize the cost function of the network \n",
    "$$\n",
    "\\omega^{*} = \\arg\\max\\limits_{\\omega} \\mathcal{C} (y, \\hat{y})\n",
    "$$\n",
    "\n",
    "However, It is not straightforward to compute the gradient of the cost function $\\mathcal{C} (y, \\hat{y})$. However, it is possible to exploit the feedforward nature of a neural network to effectively compute the gradient of the cost fucntion. In the coming lines I will present a simple derivations of the four equations that represent the working horse of the Backpropgation algorithm. We start first by defining a mathmatical object that quantifies the senstivety of the cost function to the change in the weighted input of the output layer $L$, $z_j^{L}$. Which can be also viewed as the error of $j$-th neuron in the $L$-th layer, $\\delta_j^L$. \n",
    "\n",
    "$$\n",
    "\\delta_j^L = \\frac{\\partial \\mathcal{C}}{\\partial z_j^L}\n",
    "$$\n",
    "\n",
    "In the same manner we can define the error of the $j$-th neuron in the $l$-th layer as \n",
    "$$\n",
    "\\begin{equation}\n",
    "\\delta_j^l = \\frac{\\partial \\mathcal{C}}{\\partial z_j^l} \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Using the chain rule we could write the error $\\delta_j^l$ as\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\delta_j^l = \\frac{\\partial \\mathcal{C}}{\\partial a_j^l}\\frac{\\partial a_j^l}{\\partial z_j^l} = \\frac{\\partial \\mathcal{C}}{\\partial a_j^l}\\sigma'(z_j^l)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$\\sigma'(z_j^l)$ corresponds to the dervative of the activation function with respect to the weighted input. The previous equation represents the first of the four backpropgation equations. \n",
    "\n",
    "Similarly, we can use the chain rule again and show that the error $\\delta_j^l$ could be viewed as the senstivety of the cost function for the changes in the bias  of the $j$-th neuron residing in the $l$-th layer.\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\delta_j^l = \\frac{\\partial \\mathcal{C}}{\\partial b_j^l}\\frac{\\partial b_j^l}{\\partial z_j^l} = \\frac{\\partial \\mathcal{C}}{\\partial b_j^l}\n",
    "\\end{equation}\n",
    "$$\n",
    "This is the second equation :) \n",
    "\n",
    "Since the cost function depends explicitly on the output of the last layer, the error could seen as propgating from the last layer to the inner most layers of the network. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta_j^l &= \\sum\\limits_{k}\\frac{\\partial \\mathcal{C}}{\\partial z_k^{l+1}}\\frac{\\partial z_k^{l+1}}{\\partial z_j^l}\\\\\n",
    "           &= \\sum\\limits_{k}\\delta_j^{l+1}\\frac{\\partial z_k^{l+1}}{\\partial z_j^l}\\\\\n",
    "           &= \\sum\\limits_{k}\\left( \\delta_j^{l+1}\\omega_{kj}^{l+1} \\right)\\sigma'(z_j^l)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This is the third equation, bare with me we still have one more. \n",
    "\n",
    "The last one defines the dervative of the cost function with repsect to weight connecting the $k$-th neuron at layer $l-1$ with the $j$-th neurons at the layer $l$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{C}}{\\partial \\omega_{jk}^{l}} = \\frac{\\partial \\mathcal{C}}{\\partial z_j^{l}}\\frac{\\partial z_j^{l}}{\\partial \\omega_{jk}^{l}} = \\delta_{j}^{l} a_k^{l-1}\n",
    "$$\n",
    "\n",
    "The alogrithm work in the following manner \n",
    "\n",
    "1. First compute all the activations of the input layer $a_j^0$ \n",
    "2. Feedforward, using the fact that\n",
    "$$\n",
    "a_j^l = \\sigma\\left(\\sum\\limits_{k} \\omega_{jk}^l a_j^{l-1} + b_j^l \\right)\n",
    "$$\n",
    "Compute the activations of all the subsequent layers\n",
    "3. Compute the error at the top layer $\\delta_j^L$ \n",
    "4. Backpropgate the error: use the third equation to compute the errors in the rest of the layers\n",
    "5. Finally, compute the gradient using the second and fourth equation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1081,
   "id": "ecf21fdb-7486-4469-8f18-561139715b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I start first by importing necessary libraries \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import random \n",
    "import itertools\n",
    "from tqdm import tqdm \n",
    "# Try fixing the precision :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "63c437ba-1c0a-486b-83b0-a170529121d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "# ! pip install tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1082,
   "id": "cac26d5a-ea87-4c14-a947-2ddc2beaa74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, sizes:list=[6, 2, 1]):\n",
    "        # self.seed = np.random.randint(0, 100)\n",
    "        seed = 33 # Seed that will reproduce the results \n",
    "        np.random.seed(seed)  # Set seed for reproducibility\n",
    "        self.n_layers = len(sizes) # Getting the number of layers \n",
    "        self.sizes = sizes # Getting the number of neuron per layers \n",
    "        \n",
    "        \"\"\"\n",
    "            Intilizing the weights and the biases following a standard normal distrubution \n",
    "        \"\"\"\n",
    "        self.biases  =  [np.random.randn(y, 1).astype(np.float64) for y in sizes[1:]] \n",
    "        # self.biases  =  [np.ones((y, 1)).astype(np.float64) for y in sizes[1:]] \n",
    "\n",
    "        self.weights = [np.random.randn(y, x).astype(np.float64) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        \n",
    "        # self.weights = [np.random.uniform(-3, 3, (y, x)).astype(np.float64) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feed_forward(self, a):\n",
    "        \"\"\"\n",
    "        Feed-Forward step \n",
    "        \"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = self.sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "        \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        \n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data) # Number of training points \n",
    "        for j in tqdm(range(epochs), desc=\"Training Epochs\", unit=\" Epochs\"):\n",
    "            random.shuffle(training_data) # Shuffle the training data \n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "\n",
    "            for mini_batch in mini_batches: \n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data: \n",
    "                if j % 10 == 0:\n",
    "                    print(f\"Epoch {j} : {self.evaluate(test_data)} /{n_test}\")\n",
    "            # else:\n",
    "            #     # pass\n",
    "            #     if j % 10 == 0:\n",
    "            #         print(f\"Epoch {j} complete\")\n",
    "    def GD(self, training_data, epochs, eta, test_data=None):\n",
    "        if test_data: n_test =len(test_data)\n",
    "        n = len(training_data)\n",
    "\n",
    "        for j in tqdm(np.arange(epochs)):\n",
    "            self.update(training_data, eta)\n",
    "            # for x in training_data:\n",
    "            #     self.update(x, eta)\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"\n",
    "            Function to update the mini batch \n",
    "        \"\"\"\n",
    "        \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases] # Intilizing the gradient with respect to the biases \n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights] # Intilizing the gradient with respect the weights \n",
    "        \n",
    "        for x, y in mini_batch:\n",
    "            \n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            \n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            \n",
    "        self.weights = [w - (eta/len(mini_batch)) * nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b - (eta/len(mini_batch)) * nb for b, nb in zip(self.biases, nabla_b)]\n",
    "    \n",
    "    def update(self, training_data, eta):\n",
    "        \"\"\"\n",
    "        Function to update weights and biases using full batch gradient descent\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        for x, y in training_data:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        \n",
    "        self.weights = [w - (eta / len(training_data)) * nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b - (eta / len(training_data)) * nb for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]  # Gradient w.r.t. biases\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]  # Gradient w.r.t. weights\n",
    "    \n",
    "        # Feedforward\n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        zs = []\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)  # Fixed issue here\n",
    "            activation = self.sigmoid(z)\n",
    "            activations.append(activation)\n",
    "    \n",
    "        # Backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * self.dsigmoid(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].T)\n",
    "    \n",
    "        for l in range(2, self.n_layers):\n",
    "            z = zs[-l]\n",
    "            sp = self.dsigmoid(z)\n",
    "            delta = np.dot(self.weights[-l+1].T, delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].T)\n",
    "    \n",
    "        return (nabla_b, nabla_w)\n",
    "        \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [np.argmax(self.feed_forward(x), y) for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "    def cost_derivative(self, output, y):\n",
    "        return (output-y)\n",
    "        \n",
    "    def sigmoid(self, x): \n",
    "        \"\"\"\n",
    "            Activation function \n",
    "        \"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "    def dsigmoid(self, x): \n",
    "        \"\"\"\n",
    "        Derivative of the activation function with respect to the input \n",
    "        \"\"\"\n",
    "        return self.sigmoid(x) * (1.0 - self.sigmoid(x))\n",
    "    def predict(self, x):\n",
    "        output = self.feed_forward(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab31225-ddb0-44ab-9d95-1a519d2a02f1",
   "metadata": {},
   "source": [
    "## Application\n",
    "\n",
    "The rest of the notebook will attempt to repoduce the results of the paper learning representations with backpropgation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1178,
   "id": "4e365946-70f3-4157-b841-fb15fab43eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "\n",
    "# An attempt to reproduce the results of https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf\n",
    "# Generating the data \n",
    "def is_symmetric(seq):\n",
    "    return seq == seq[::-1]\n",
    "m = 6\n",
    "# Generate all 6-bit sequences and labels\n",
    "all_sequences = list(itertools.product([0, 1], repeat=m))\n",
    "labels = [1 if is_symmetric(seq) else 0 for seq in all_sequences]\n",
    "training_data = [(np.array(seq, dtype=np.float32).reshape(m, 1), \n",
    "                  np.array([label], dtype=np.float32).reshape(1, 1)) \n",
    "                 for seq, label in zip(all_sequences, labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1179,
   "id": "a570aa86-4aa1-4325-9d07-7206265d7487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are: 64 datapoints\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are: {len(training_data)} datapoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "24593034-3172-4bf8-9f2a-0412261411eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size = int(0.8 * len(training_data))\n",
    "# train_set = training_data[:train_size]\n",
    "# test_set = training_data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1180,
   "id": "2b123e67-551f-4a2b-9247-76cf9d43b8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining neural network \n",
    "sizes = [m, 2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1181,
   "id": "1ecec823-7e4c-480b-b4ac-727b37b76500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [00:02<00:00, 198.08 Epochs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights of the first hidden layer\n",
      "[[-11.2  11.3]\n",
      " [  2.8  -2.9]\n",
      " [  5.6  -5.7]\n",
      " [ -5.6   5.7]\n",
      " [ -2.8   2.9]\n",
      " [ 11.2 -11.3]]\n",
      "The biases of the hidden layer\n",
      "[[-2. -2.]]\n",
      "The bias of the output layer\n",
      "[[5.5]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net = Network(sizes)\n",
    "eta = 1.0\n",
    "epochs = 500\n",
    "batch_size = 1\n",
    "net.SGD(training_data ,epochs, batch_size, eta, test_data=None)\n",
    "print(\"The weights of the first hidden layer\")\n",
    "print(net.weights[0].transpose().round(1))\n",
    "print(\"The biases of the hidden layer\")\n",
    "print(net.biases[0].transpose().round(1))\n",
    "print(\"The bias of the output layer\")\n",
    "print(net.biases[1].transpose().round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d00e397-3d15-46c7-8c83-17b3fd792c58",
   "metadata": {},
   "source": [
    "# Remarks \n",
    "1. Using $\\varepsilon=0.9$ with `batch_size=6` and `Epochs=1425`, and weights and biases as standard normal distros it is possible to generate the alternating sign effect\n",
    "2. epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fedd26-3839-4241-a50c-29d5b72aa820",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
