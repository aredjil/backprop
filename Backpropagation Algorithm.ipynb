{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e287fe1-612a-4a4d-a3fa-d1a13d72210c",
   "metadata": {},
   "source": [
    "# Backpropagation Algorithm\n",
    "## Notes for Myself \n",
    "- [ ] Add the section on Gradient Descent\n",
    "- [ ] Improve the section on Backpropgation\n",
    "- [ ] Fix the typos and write better sentences \n",
    "\n",
    "## Introduction \n",
    "\n",
    "The aim of supervised machine learning is to develop algorithms that enable machines to implicitly learn pattrens from data. The data used in supervised learning is labeled, in other words the machine is feed examples about the pattrens to learn. $\\mathcal{D}=\\{(\\mathbf{X}_i,y_i)\\}_{i\\leq N}$\n",
    "- $\\mathbf{X}_i$: Is a $p$-dimensional vector that represents the observation, $p$ is the number of features of each observation.\n",
    "- $y_i$: is the associated label or target to the observation $\\mathbf{X}_i$\n",
    "\n",
    "The problem of supervised learning depends on the nature of the target, if the target $y_i$ is continiuous we are in the case of regression, where if it is discrete we are in a classification setting. \n",
    "\n",
    "One of the most prominent techniques to perform supervised machine learning is the so called neural networks, where the input is propagated through a cascade of layers made of several neurons, each layer adds non-linearity to it's input and passes it to the next layer. Neural networks are made of three parts: input layer, hidden layers and output layer. The input layer has $p$ neurons each neuron holds the value of a feature, while the hidden layers have varied number of neurons, and the output layer depends on the task, in the case of binary classification the last layer has one neuron, while in multiclassifcation the number of neurons corresppnds to the number of classes to be infered. \n",
    "\n",
    "Neural netowrks are a family of functions parameterized with their weights, the end game of supervised machine learning is to identify the network with weights the best fit the example data, but also is able to generlize on unseen data. \n",
    "\n",
    "The process of searching for the optimal weights for the network is called training, during which the weights are adjusted iteratively untill the best fit is reached. To quantify how well the network is doing, each change in the weights is quantified using an object called the cost function $\\mathcal{C}(\\mathbf{y}, \\hat{\\mathbf{y}})$ that measures how far the network's prediction $\\hat{y_i}$ is from the actual value $y_i$. The choice of the cost function depends on the task in hand, a usual choice is the mean-square error\n",
    "$$\n",
    "\\mathcal{C}(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\frac{1}{2} \\sum\\limits_{i=1}^{N}\\lVert y_i - \\hat{y}_i\\rVert^2\n",
    "$$\n",
    "The optimal set of weights $\\{\\omega_{jk}^l\\}$ is the one that minmizes the value of the cost function. \n",
    "\n",
    "$$\n",
    "\\omega^{*} = \\arg\\max\\limits_{\\omega}\\mathcal{C}(\\mathbf{y}, \\hat{\\mathbf{y}})\n",
    "$$\n",
    "\n",
    "In order to identify the optimal weights for a network the method of Gradient Descent or one of it's generlizations is employed [see](https://en.wikipedia.org/wiki/Gradient_descent). \n",
    "\n",
    "**Backpropagation Algorithm:**\n",
    "\n",
    "- [x] Add the defintions of the weights and the activation ..etc..\n",
    "\n",
    "Before I start with the derivation of the algorithm, let us establish some operational definitions to helps navigate the problem.\n",
    "- $\\omega_{jk}^l$ : The weight that connects the $k$-th neurons from the $l$ layer to the $j$-th neuron in the layer $l-1$\n",
    "- $a_j^l$: The activation of the $j$-th neuron in the layer $l$, such that\n",
    "$$\n",
    "a_j^l = \\sigma\\left(z_j^l\\right)\n",
    "$$\n",
    "- $z_j^l$: is the weighted sum of the input of layer $l$ defined by:\n",
    "  $$\n",
    "  z_j^l = \\sum\\limits_{k} \\omega_{jk}^l a_j^{l-1} + b_j^l \n",
    "  $$\n",
    "- $b_j^l$: is the bias of the $j$-th neuron in layer $l$\n",
    "Gradient Decent allows us to identify the set of weights which optimize the cost function of the network.\n",
    "$$\n",
    "\\omega^{*} = \\arg\\max\\limits_{\\omega} \\mathcal{C} (y, \\hat{y})\n",
    "$$\n",
    "However, It is not straightforward to compute the gradient of the cost function $\\mathcal{C} (y, \\hat{y})$. However, it is possible to exploit the feedforward nature of a neural network to effectively compute the gradient of the cost fucntion. In the coming lines I will present a simple derivations of the four equations that represent the working horse of the Backpropgation algorithm. We start first by defining a mathmatical object that quantifies the senstivety of the cost function to the change in the weighted input of the output layer $L$, $z_j^{L}$. Which can be also viewed as the error of $j$-th neuron in the $L$-th layer, $\\delta_j^L$. \n",
    "\n",
    "$$\n",
    "\\delta_j^L = \\frac{\\partial \\mathcal{C}}{\\partial z_j^L}\n",
    "$$\n",
    "\n",
    "In the same manner we can define the error of the $j$-th neuron in the $l$-th layer as \n",
    "$$\n",
    "\\begin{equation}\n",
    "\\delta_j^l = \\frac{\\partial \\mathcal{C}}{\\partial z_j^l} \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Using the chain rule we could write the error $\\delta_j^l$ as\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\delta_j^l = \\frac{\\partial \\mathcal{C}}{\\partial a_j^l}\\frac{\\partial a_j^l}{\\partial z_j^l} = \\frac{\\partial \\mathcal{C}}{\\partial a_j^l}\\sigma'(z_j^l)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "$\\sigma'(z_j^l)$ corresponds to the dervative of the activation function with respect to the weighted input. The previous equation represents the first of the four backpropgation equations. \n",
    "\n",
    "Similarly, we can use the chain rule again and show that the error $\\delta_j^l$ could be viewed as the senstivety of the cost function for the changes in the bias  of the $j$-th neuron residing in the $l$-th layer.\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\delta_j^l = \\frac{\\partial \\mathcal{C}}{\\partial b_j^l}\\frac{\\partial b_j^l}{\\partial z_j^l} = \\frac{\\partial \\mathcal{C}}{\\partial b_j^l}\n",
    "\\end{equation}\n",
    "$$\n",
    "This is the second equation :) \n",
    "\n",
    "Since the cost function depends explicitly on the output of the last layer, the error could seen as propgating from the last layer to the inner most layers of the network. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta_j^l &= \\sum\\limits_{k}\\frac{\\partial \\mathcal{C}}{\\partial z_k^{l+1}}\\frac{\\partial z_k^{l+1}}{\\partial z_j^l}\\\\\n",
    "           &= \\sum\\limits_{k}\\delta_j^{l+1}\\frac{\\partial z_k^{l+1}}{\\partial z_j^l}\\\\\n",
    "           &= \\sum\\limits_{k}\\left( \\delta_j^{l+1}\\omega_{kj}^{l+1} \\right)\\sigma'(z_j^l)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This is the third equation, bare with me we still have one more. \n",
    "\n",
    "The last one defines the dervative of the cost function with repsect to weight connecting the $k$-th neuron at layer $l-1$ with the $j$-th neurons at the layer $l$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{C}}{\\partial \\omega_{jk}^{l}} = \\frac{\\partial \\mathcal{C}}{\\partial z_j^{l}}\\frac{\\partial z_j^{l}}{\\partial \\omega_{jk}^{l}} = \\delta_{j}^{l} a_k^{l-1}\n",
    "$$\n",
    "\n",
    "The alogrithm work in the following manner \n",
    "\n",
    "1. First compute all the activations of the input layer $a_j^0$ \n",
    "2. Feedforward, using the fact that\n",
    "$$\n",
    "a_j^l = \\sigma\\left(\\sum\\limits_{k} \\omega_{jk}^l a_j^{l-1} + b_j^l \\right)\n",
    "$$\n",
    "Compute the activations of all the subsequent layers\n",
    "3. Compute the error at the top layer $\\delta_j^L$ \n",
    "4. Backpropgate the error: use the third equation to compute the errors in the rest of the layers\n",
    "5. Finally, compute the gradient using the second and fourth equation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecf21fdb-7486-4469-8f18-561139715b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I start first by importing necessary libraries \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import random \n",
    "import itertools\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cac26d5a-ea87-4c14-a947-2ddc2beaa74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, sizes:list=[6, 2, 1]):\n",
    "        # self.seed = np.random.randint(0, 100) # Generating random seeds. Pichking the one that reproduces similar results\n",
    "        seed = 33 # Seed that will reproduce the results \n",
    "        np.random.seed(seed )  # Set seed for reproducibility\n",
    "        self.n_layers = len(sizes) # Getting the number of layers \n",
    "        self.sizes = sizes # Getting the number of neuron per layers \n",
    "        \n",
    "        # Intilizing the biases and weights to standard normal values \n",
    "        self.biases  =  [np.random.randn(y, 1).astype(np.float32) for y in sizes[1:]] \n",
    "\n",
    "        self.weights = [np.random.randn(y, x).astype(np.float32) for x, y in zip(sizes[:-1], sizes[1:])]        \n",
    "\n",
    "    def feed_forward(self, a:np.ndarray)->np.ndarray:\n",
    "        \"\"\"\n",
    "        Feed forward step, updates the activations of all the layers\n",
    "        \"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = self.sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "        \n",
    "    def SGD(self, training_data:np.ndarray, epochs:int, mini_batch_size:np.ndarray, eta:float, test_data:np.ndarray=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        A function that performs Stochstic Gradient Descent (SGD) optimization to the cost function \n",
    "        \n",
    "        \"\"\"\n",
    "        if test_data: n_test = len(test_data) # If the test set is provided get its size\n",
    "        n = len(training_data) # Get the size of the training set\n",
    "        # tqdm method visulizes a nice looking progrees bar \n",
    "        for j in tqdm(range(epochs), desc=\"Training Epochs\", unit=\" Epochs\"): # Iterate over the number of epochs \n",
    "            \n",
    "            random.shuffle(training_data) # Shuffle the training data \n",
    "            # Construct mini-batchs using the training data \n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "\n",
    "            for mini_batch in mini_batches:  # Iterate over the mini batchs \n",
    "                self.update_mini_batch(mini_batch, eta) # Perform SGD on the weights using a signle batch   \n",
    "            if test_data: # If the testset is provided evaluate the performance of the model  \n",
    "                if j % 10 == 0: # Every ten epochs \n",
    "                    print(f\"Epoch {j} : {self.evaluate(test_data)} /{n_test}\")\n",
    "            # else:\n",
    "            #     # pass\n",
    "            #     if j % 10 == 0:\n",
    "            #         print(f\"Epoch {j} complete\")\n",
    "    def GD(self, training_data, epochs, eta, test_data=None):\n",
    "        \"\"\"\n",
    "            Method to perform Gradient Decent (GD) optimization to the cost function \n",
    "        \"\"\"\n",
    "        if test_data: n_test =len(test_data) # Get the size of the testset\n",
    "        n = len(training_data) # Get the size of the training set\n",
    "\n",
    "        for j in tqdm(np.arange(epochs)): # Iterate over epochs \n",
    "            self.update(training_data, eta) # Update the weights using Gradient Descent. \n",
    "            # for x in training_data:\n",
    "            #     self.update(x, eta)\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"\n",
    "            Method to update the weights of the network using SGD  \n",
    "        \"\"\"\n",
    "        \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases] # Intilizing the gradient with respect to the biases to zeros \n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights] # Intilizing the gradient with respect the weights to zeros \n",
    "        # Iterate over the observations and the targets in the current mini batch \n",
    "        for x, y in mini_batch:\n",
    "            \n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y) # Get the errors with respect to the biases and weights \n",
    "                                                               # using the second and fourth equations of\n",
    "                                                               #  backpropgation,\n",
    "            \n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] # Compute the Gradient component with respect to the biases\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] # Compute the gradient component with respect to the weights\n",
    "            \n",
    "        # self.weights = [w - (eta/len(mini_batch)) * nw for w, nw in zip(self.weights, nabla_w)] # Update the weights using GD\n",
    "        # self.biases = [b - (eta/len(mini_batch)) * nb for b, nb in zip(self.biases, nabla_b)] # Update the biases using GD \n",
    "        \n",
    "        self.weights = [w - (eta) * nw for w, nw in zip(self.weights, nabla_w)] # Update the weights using GD\n",
    "        self.biases = [b - (eta) * nb for b, nb in zip(self.biases, nabla_b)] # Update the biases using GD \n",
    "\n",
    "    def update(self, training_data, eta):\n",
    "        \"\"\"\n",
    "        Function to update weights and biases using full batch gradient descent\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        for x, y in training_data:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        \n",
    "        self.weights = [w - (eta / len(training_data)) * nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b - (eta / len(training_data)) * nb for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"\n",
    "        Method to perform backpropgation step \n",
    "        \"\"\"\n",
    "        \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]  # Gradient w.r.t. biases\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]  # Gradient w.r.t. weights\n",
    "    \n",
    "        # Feedforward pass \n",
    "        # Compute the activation of the first layer \n",
    "        # Using the recursive relation of the activation compute \n",
    "        # the activations of the subsequent layers  \n",
    "        activation = x # Activation of the input layer is the input \n",
    "        activations = [x] # Intilize a list to store activations of the layers\n",
    "        zs = [] # Empty list to store all the weighted sums of activations \n",
    "        for b, w in zip(self.biases, self.weights):# Iterate over the biases and the weights of the model \n",
    "            z = np.dot(w, activation) + b # Compute the weighted sum of the activation \n",
    "            zs.append(z)  # Add the weighted sum to the list \n",
    "            activation = self.sigmoid(z) # Compute the activation \n",
    "            activations.append(activation) # Update the list of activations \n",
    "    \n",
    "        # Backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * self.dsigmoid(zs[-1]) # Computing the error using the first backprop equation \n",
    "        nabla_b[-1] = delta # Setting the gradient with respect to the bias to be the error since it is by definition of the output layer \n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].T) # Cimputing the gradient with respect to the weights using the fourth equation of the output layer\n",
    "    \n",
    "        for l in range(2, self.n_layers): # Recursively computing the gradient in shell layers \n",
    "            z = zs[-l] # Getting the weighted sum of the inputs \n",
    "            sp = self.dsigmoid(z) # Computing the activation of the corresponding layer\n",
    "            delta = np.dot(self.weights[-l+1].T, delta) * sp # Computing the error \n",
    "            nabla_b[-l] = delta # Setting the gradient with respect to the bias \n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].T) # Setting the gradient with respect to the weights \n",
    "    \n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [np.argmax(self.feed_forward(x), y) for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "    def cost_derivative(self, output, y):\n",
    "        return (output-y)\n",
    "        \n",
    "    def sigmoid(self, x): \n",
    "        \"\"\"\n",
    "            Activation function \n",
    "        \"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "    def dsigmoid(self, x): \n",
    "        \"\"\"\n",
    "        Derivative of the activation function with respect to the input \n",
    "        \"\"\"\n",
    "        return self.sigmoid(x) * (1.0 - self.sigmoid(x))\n",
    "    def predict(self, x):\n",
    "        output = self.feed_forward(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab31225-ddb0-44ab-9d95-1a519d2a02f1",
   "metadata": {},
   "source": [
    "## Application\n",
    "\n",
    "The rest of the notebook will attempt to repoduce the results of the paper learning representations with backpropgation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e365946-70f3-4157-b841-fb15fab43eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An attempt to reproduce the results of https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf\n",
    "# Generating the data \n",
    "def is_symmetric(seq:list)->bool:\n",
    "    # seq[::-1] creates a reversed version of the input list seq\n",
    "    return seq == seq[::-1]\n",
    "\n",
    "m = 6 # Number of elements of the vector  \n",
    "# Compute the cartesian product of the the set {0, 1} repeated m times \n",
    "# To generate all the possible ombinations of 0 and 1 of length m \n",
    "all_sequences = list(itertools.product([0, 1], repeat=m))\n",
    "# Assign the label 1 to the symmteric vectors and 0 to the non-symmteric. \n",
    "labels = [1 if is_symmetric(seq) else 0 for seq in all_sequences]\n",
    "# Combine the observations and labels in a numpy array \n",
    "training_data = [(np.array(seq, dtype=np.float32).reshape(m, 1), \n",
    "                  np.array([label], dtype=np.float32).reshape(1, 1)) \n",
    "                 for seq, label in zip(all_sequences, labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a570aa86-4aa1-4325-9d07-7206265d7487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are: 64 datapoints\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are: {len(training_data)} datapoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24593034-3172-4bf8-9f2a-0412261411eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size = int(0.8 * len(training_data))\n",
    "# train_set = training_data[:train_size]\n",
    "# test_set = training_data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b123e67-551f-4a2b-9247-76cf9d43b8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining neural network \n",
    "sizes = [m, 2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ecec823-7e4c-480b-b4ac-727b37b76500",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epochs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1425/1425 [00:02<00:00, 542.14 Epochs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights of the first hidden layer\n",
      "[[-13.9  13.9]\n",
      " [  3.5  -3.5]\n",
      " [  7.   -7. ]\n",
      " [ -7.    7. ]\n",
      " [ -3.5   3.5]\n",
      " [ 13.9 -13.9]]\n",
      "The weights of the output layer\n",
      "[[-12.7]\n",
      " [-12.7]]\n",
      "The biases of the hidden layer\n",
      "[[-2.3 -2.3]]\n",
      "The bias of the output layer\n",
      "[[5.8]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net = Network(sizes)\n",
    "eta = 0.9\n",
    "epochs = 1425\n",
    "batch_size = len(training_data)\n",
    "net.SGD(training_data ,epochs, batch_size, eta, test_data=None)\n",
    "print(\"The weights of the first hidden layer\")\n",
    "print(net.weights[0].transpose().round(1))\n",
    "print(\"The weights of the output layer\")\n",
    "print(net.weights[1].transpose().round(1))\n",
    "print(\"The biases of the hidden layer\")\n",
    "print(net.biases[0].transpose().round(1))\n",
    "print(\"The bias of the output layer\")\n",
    "print(net.biases[1].transpose().round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fedd26-3839-4241-a50c-29d5b72aa820",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
